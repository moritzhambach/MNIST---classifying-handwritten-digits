{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Network from scratch + Comparison to Keras\n",
    "while we can build neural networks (NN) with only a few lines of code using high level Python packages such as tensorflow and keras, building one from scratch helps making sure we understand everything. I will try to write efficient, vectorised code instead of the much slower for/while loops. The datasets could be anything, so we start with the famous MNIST set of handwritten digits and build a classifier. The result will then be compared to a Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np                                \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#import tensorflow as tf\n",
    "# import sklearn as sk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. The \"one_hot=True\" options means the label (= the true number) will be represented as a vector with a 1 at the corresponding position, e.g a \"2\" will be given as [0,0,1,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already separated into m_train train images and m_test test images with corresponding labels. \n",
    "The images are in greyscale (values 0-1) and are given as a one-dimensional vector with length n_pix (=number of pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pix=784, m_train=55000, m_test=10000\n"
     ]
    }
   ],
   "source": [
    "n_pix=len(mnist.train.images[0])\n",
    "m_train=len(mnist.train.images)\n",
    "m_test=len(mnist.test.images)\n",
    "print('n_pix='+str(n_pix)+', '+ 'm_train='+str(m_train)+', '+ 'm_test='+str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can show the images by reshaping them into (28x28) arrays and use the io package to display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAABVCAYAAAAcyXCzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE5SIiERJJygREYnS/wP0GSfxMruoLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa702757f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "piclist=[]\n",
    "for j in range(5):\n",
    "    pic=mnist.train.images[j].reshape((28,28))\n",
    "    piclist.append(pic)\n",
    "io.imshow_collection(piclist)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 4, 6, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerlist=[]\n",
    "for j in range(5):\n",
    "    answer=np.where(mnist.train.labels[j]==1)[0].item()    \n",
    "    answerlist.append(answer)\n",
    "answerlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neural Network geometry\n",
    "The geometry of a simple NN is shown below. It consists of an input layer (in our case the 784 pixel values), followed by a number of hidden layers and finally an output layer (with in our case $K=10$ neurons representing the 10 possible digits). Following the deep learning course by Andrew Ng on coursera, the total number of layers (not counting the input) is called $L$, with $s_l$ neurons in layer $l=0...L$, and $s_L=K$ neurons in the output layer. The neurons have values called activations $a^{[l]}$, starting with the input values $a^{[0]}$, the pixel values of our image. To calculate the activation vector $a^{[l]}$ of the neurons in layer $l$, we first multiply the activation vector of the previous layer $a^{[l-1]}$ with the corresponding weight matrix $W^{[l]}$ and add a bias vector $b^{[l]}$, to produce $z^{(l)}= W^{[l]} a^{[l-1]}+b^{[l]}$. The matrices $W^{[l]}$ and the bias vectors $b^{[l]}$ represent the parameters that will be learned through training of the network.\n",
    "\n",
    "The resulting vector $z^{[l]}$ is fed into a nonlinear activation function to give the activation of the neurons of the next layer, $a^{[l]}=g(z^{[l]})$. Historically, the sigmoid function $\\sigma(z)=\\frac{1}{1+exp(-z)}$ was used, or alternatively $tanh(z)$, but recently the computationally easier RELU function $r(z)=max(0,z)$ is the preferred choice for all but the output layer. Here often the softmax function $s(z)_j=\\frac{exp(z_j)}{\\sum_{k=1}^K exp(z_k)}$ is used, which is conveniently normalised ($\\sum s(z)_j=1$) and will be interpreted as probability that the input image belongs to a class.\n",
    "\n",
    "Instead of looping through all $m$ training examples $x^{(i)}$, I will vectorise the process to be more efficient: The data points $x^{(i)}$ will be stored as columns in a matrix $A^{[0]}$, and the corresponding $z$-values will be the columns of the matrix $Z^{[l]}$, with $Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z=np.clip( z, -500, 500 )                              #restrict z to (-500,500) to avoid overflow\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z=np.clip( z, -500, 500 )                              #restrict z to (-500,500) to avoid overflow\n",
    "    ex=np.exp(z)                                            \n",
    "    return ex / np.sum(ex, axis=0,keepdims=True)           #column wise sum (collapse column vector to number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation and feed forward\n",
    "The process described above is called feed forward, and in practice we initialise the weigth matrices of a network randomly and can feed forward to a first guess (interpreting the activations of the last layer as a measure for likelihoood of the corresponding outcome). We will use either sigmoid or relu as activation in all hidden layers and sigmoid or softmax in the output layer. The list \"sizes\" will contain the numbers of neurons in each layer, $(s_0, ..., s_L)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes=[784,40,10]  # one hidden layer with 40 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialise(sizes):\n",
    "    weights_list=[]\n",
    "    biases_list=[]\n",
    "    sig=np.sqrt(2/sizes[0])                                            # choose small standard deviation\n",
    "    gauss=np.random.randn\n",
    "    for k in range(len(sizes)-1):\n",
    "        w=sig*gauss(sizes[k+1], sizes[k])                              # random numbers from gaussian\n",
    "        b=np.zeros((sizes[k+1], 1))            # setting biases to zero is ok, as symmetry is broken already\n",
    "        weights_list.append(w)\n",
    "        biases_list.append(b)\n",
    "    return [weights_list, biases_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[weights_list, biases_list]=initialise(sizes)                                #inialise\n",
    "#(weights_list[0].shape,biases_list[0].shape,weights_list[1].shape,biases_list[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feedforward(w_list, b_list, A_in, activation, activation_final):\n",
    "    A=A_in.T\n",
    "    n_h=len(w_list)-1\n",
    "    Alist=[A]\n",
    "    Zlist=[]\n",
    "    for k in range(n_h):\n",
    "        Z=np.dot(w_list[k], A)+b_list[k]                #add same vector b to each column of matrix (\"broadcasting\")\n",
    "        A = activation(Z)\n",
    "        Zlist.append(Z)\n",
    "        Alist.append(A)\n",
    "    Z=np.dot(w_list[-1], A)+b_list[-1]\n",
    "    A = activation_final(Z)                             #softmax as final layer\n",
    "    Alist.append(A)\n",
    "    Zlist.append(Z)\n",
    "    return [Zlist,Alist]                                #return the activations and z's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 55000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedtest=feedforward(weights_list, biases_list, mnist.train.images, relu, softmax)   #testing feed forward\n",
    "feedtest[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function + regularisation\n",
    "Once the activation of the final (=output) layer is calculated, we can compare it to the true label and define a loss or cost function, describing how far off we are. The aim will be to minimise this cost function. The total cost over the training (or test) set is the average of the cost terms $J$ of each data point $(x^{(i)},y^{(i)})$, $J_{tot}=\\frac{1}{m}\\sum_m J$.\n",
    "\n",
    "There are different choices for the cost, for example the cross entropy $J=-\\sum_k y_k\\,log(a_k)+ (1-y_k)\\,log((1-a_k))$, where $a$ is the activation of the output layer and $y$ is the true label vector, with a one marking the correct answer.\n",
    "\n",
    "To prevent overfitting, we regularise the cost by adding a term $\\frac{\\lambda}{2 m}\\sum_l |W^{[l]}|^2$, which sums up the squared weights of all connections in the network, effectively penalising the algorithm for using large weights. The parameter $\\lambda$ should be chosen carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(a,y, weights_list, lmbda):\n",
    "    m=len(y)                                        \n",
    "    y=y.T\n",
    "    J=-1/m*np.sum(np.multiply(y,np.log(a))+np.multiply((np.ones(y.shape)-y),np.log(np.ones(a.shape)-a)))\n",
    "    for w in weights_list:\n",
    "        J=J+lmbda/(2*m)*(np.sum(np.square(w)))                     #regularisation term (L2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2713185933856641"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(feedtest[1][-1],mnist.train.labels,weights_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(x, y, Wlist, blist,activation, activation_final):  #check prediction accuracy on test (or training) data\n",
    "    A_final=feedforward(Wlist, blist, x, activation, activation_final)[1][-1]\n",
    "    predict_list=A_final.argmax(axis=0)\n",
    "    true_list=y.T.argmax(axis=0)\n",
    "    return sum(predict_list==true_list)/len(predict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network: Back propagation and gradient descent\n",
    "We now minimise the cost by calculating the gradient of the cost with respect to the weights and biases. For this, we change the weights and biases proportional to a learning rate $\\alpha$ in the direction of the partial gradients of the cost function: $W^{[l]}\\rightarrow W^{[l]}- \\alpha\\frac{\\partial J}{ \\partial W^{[l]}}$ and $b^{[l]}\\rightarrow b^{[l]}- \\alpha \\frac{\\partial J}{ \\partial b^{[l]}}$. The process can be compared with trying to get to the top of a mountain by always taking a step (with step size $\\alpha$) in the steepest direction. If $\\alpha$ is too large we might step too far, but if $\\alpha$ is too small, it will take forever and we might get stuck on a small hill.\n",
    "\n",
    "So how do we get the gradients? Just like we found the final activation layer by propagating forward through the layers, we now propagate backwards to get all derivatives: We start with the last layer, $\\frac{\\partial J}{ \\partial W^{[L]}}=\\frac{\\partial J}{ \\partial z^{[L]}}\\frac{\\partial z^{[L]}}{ \\partial W^{[L]}}$ with $\\frac{\\partial z^{[L]}}{ \\partial W^{[L]}}=a^{[L-1]}$ and the \"error\" of layer $L$, defined as $\\delta^{[L]}:=\\frac{\\partial J}{ \\partial z^{[L]}}$. All partial derivatives with respect to a vector are to be understood elementwise (e.g. $\\delta_j^{[L]}:=\\frac{\\partial J}{ \\partial z_j^{[L]}}$), and so is the multiplication between the vectors.\n",
    "\n",
    "Next, we calculate the errors from $\\delta^{[L]}=\\frac{\\partial J}{ \\partial a^{[L]}}\\frac{\\partial a^{[L]}}{ \\partial z^{[L]}}$ with $\\frac{\\partial J}{ \\partial a_k}=\\frac{\\partial }{ \\partial a_k} \\left( -\\sum_{k'} y_{k'}\\,log(a_{k'})+ (1-y_{k'})\\,log((1-a_{k'})) \\right)=-\\frac{y_k}{a_k}+\\frac{1-y_k}{1-a_k}=\\frac{a_k-y_k}{a_k(1-a_k)}$ for cross entropy and $\\frac{\\partial a^{[L]}}{\\partial z^{[L]}}=\\frac{\\partial}{ \\partial z^{[L]}}g(z^{[L]})=g'(z^{[L]})$. If $g=softmax$, we get $g'(z)=softmax(z)\\,(1-softmax(z))=a \\,(1-a)$ since $a=g(z)$, and similarly if $g=sigmoid$, $g'(z)=sigmoid(z)\\,(1-sigmoid(z))=a \\,(1-a)$. In both cases the error reduces to $\\delta^{[L]}=a^{[L]}-y$.\n",
    "\n",
    "Finally, the error of layer ($l-1$) can be calculated from the error of layer $l$ via $\\delta^{[l-1]}=(W^{[l]})^T   \\delta^{[l]}\\, g'(z^{[l-1]})$, where $W^T$ is the transposed weight matrix. For the biases, we simply have $\\frac{\\partial J}{ \\partial b^{[l]}}=\\delta^{[l]}$.\n",
    "\n",
    "Another implementation detail is the Mini-batch method. In backpropagation, we calculate the gradient of the cost function with respect to $b$ or $W$ by averaging over all training examples. This can take long, so instead a common method is to select a random subset called \"mini-batch\" of $n<m$ training examples and take the average gradient of those. As a result, the algorithm will be changing the weights and biases in a sometimes less optimum way, which might in rare cases even increase the cost, but eacch step will be done much faster. Typical step size could be $n=10$, compared to $m=55000$ in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_sigmoid(z):\n",
    "    sig=sigmoid(z)\n",
    "    return sig*(1-sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_relu(z):\n",
    "    z=z>0                  #the gradient at zero is not defined, but it's ok to set it to zero\n",
    "    return z.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_softmax(z):\n",
    "    grad=np.multiply(softmax(z),np.ones(z.shape)-softmax(z))\n",
    "    return grad                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(weights_list, biases_list, A, Y, sizes, activation, activation_final, epochs, bsize, alpha, lmbda):\n",
    "    \n",
    "    L=len(sizes)-1                               #don't count the input layer\n",
    "    m=len(A)\n",
    "    \n",
    "    A_in=A\n",
    "    Y_in=Y\n",
    "    cost_list=[]\n",
    "    acc_train_list=[]\n",
    "    acc_test_list=[]\n",
    "\n",
    "    A=A.T                                        # we want each data point correspond to a column, not a row\n",
    "    Y=Y.T\n",
    "    \n",
    "    if str(activation)==str(sigmoid):            # allow two different activation functions for the hidden layers\n",
    "        grad_activation=grad_sigmoid\n",
    "    elif str(activation)==str(relu):\n",
    "        grad_activation=grad_relu\n",
    "    else: print(\"check activation\")\n",
    "    \n",
    "    test_data_norm=(mnist.test.images-np.mean(mnist.train.images))/np.std(mnist.train.images) #normalise in same way as training data\n",
    "\n",
    "    for e in range(epochs):                          #loop through forward- and backprop\n",
    "        \n",
    "        shuffled=np.vstack((A,Y))                    # shuffled: (794 x 55000)\n",
    "        np.random.shuffle(shuffled.T)                #shuffles columns of images and labels\n",
    "        Ashuffled=shuffled[0:-10,:]                  #Ashuffled (784x55000)\n",
    "        Yshuffled=shuffled[-10:,:]                   #Yshuffled (10x55000)\n",
    "        nb=int(np.floor(m/bsize))                    # number of minibatches per epoch\n",
    "        \n",
    "        for b in range(nb):\n",
    "            dWlist=[]                                 # list to save weights errors in each mini-batch run\n",
    "            dblist=[]                                 # list to save bias errors \n",
    "            A=Ashuffled[:,(b*bsize):((b+1)*bsize)]    #(784 x bsize)\n",
    "            Y=Yshuffled[:,(b*bsize):((b+1)*bsize)]    #(10 x bsize)\n",
    "            \n",
    "            #feed forward to get activations:\n",
    "            [Zlist,Alist]=feedforward(weights_list, biases_list, A.T, activation, activation_final)\n",
    "            \n",
    "            #first backprop step in output layer:      \n",
    "            dZ=Alist[-1]-Y                                            # dimensions: dZ2:(10 x bsize), A2:(10 x bsize)\n",
    "            dW=1/bsize*np.dot(dZ, Alist[-2].T)                                     #dW2:(10 x s1), A1:(s1 x bsize)\n",
    "            db=1/bsize*np.sum(dZ,axis=1,keepdims=True)                             #db2:(10 x 1)\n",
    "            dWlist.append(dW)\n",
    "            dblist.append(db)     \n",
    "                      \n",
    "            #steps through hidden layers (sigmoid or relu)\n",
    "            for j in range(L-1):\n",
    "                xz=np.dot(weights_list[-(j+1)].T,dZ)                               #xz:(s1 x bsize)                      \n",
    "                dZ=np.multiply(xz, grad_activation(Zlist[-(j+2)]))                 #dZ1:(s1 x bsize)\n",
    "                dW=1/bsize*np.dot(dZ, Alist[-(j+3)].T)                             #dW1:(s1 x 784)\n",
    "                db=1/bsize*np.sum(dZ,axis=1,keepdims=True)                                \n",
    "                dWlist.append(dW)\n",
    "                dblist.append(db)\n",
    "            \n",
    "            #update weights and biases:                                            #with L2 regularisation\n",
    "            for j in range(len(dWlist)):\n",
    "                weights_list[-(j+1)]=(1-alpha*lmbda)*weights_list[-(j+1)]-alpha*dWlist[j]\n",
    "            for k in range(len(dblist)):\n",
    "                biases_list[-(k+1)]=biases_list[-(k+1)]-alpha*dblist[k]\n",
    "            \n",
    "            #end of mini-batch loop, repeat\n",
    "            \n",
    "        #cost_list.append(cost(Alist[-1],Y.T,weights_list,lmbda))                  #get cost for plotting\n",
    "        \n",
    "        acc_train=accuracy(A_in,Y_in,weights_list, biases_list, activation, activation_final)\n",
    "        acc_test=accuracy(test_data_norm,mnist.test.labels,weights_list, biases_list, activation, activation_final)\n",
    "        acc_train_list.append(acc_train)\n",
    "        acc_test_list.append(acc_test)\n",
    "        #end of epoch, repeat\n",
    "        \n",
    "    Wlist=weights_list\n",
    "    blist=biases_list\n",
    "\n",
    "    return [Wlist, blist,acc_train_list,acc_test_list]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters, Bias and Variance\n",
    "\n",
    "Now that the algorithms are set up, we can train the model parameters (weights and biases) for different sets of hyper parameters (choice of activation function, number of neurons and hidden layers, learning rate, regularisation). I will mostly work with one hidden layer, and start without regularisation. \n",
    "\n",
    "In order to help finding the right parameters, it helps to know about the high bias/ high variance problem. In a high bias situation, the model doesn't have enough degrees of freedom to make good fits to the data (imagine fitting a linear function to quadraticly looking data). Here, additional data will not help, but instead the model should be made more complex (e.g. more neuron, more layers). If instead we have high variance, the model fits perfecctly to the training data but fails to generalise well to also classify the test data. This \"over-fitting\" can be avoided by adding more data or other techniques such as regularisation (e.g. increase $\\lambda$) or drop out.\n",
    "\n",
    "Before we start, it is preferred to have data with mean zero and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.7791406e-07, 0.99999982)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm=(mnist.train.images-np.mean(mnist.train.images))/np.std(mnist.train.images)\n",
    "test_data_norm=(mnist.test.images-np.mean(mnist.test.images))/np.std(mnist.test.images)\n",
    "np.mean(data_norm),np.std(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sizes=[784,300,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result depends on the initialised weigths, so we average over a few runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.59 s\n"
     ]
    }
   ],
   "source": [
    "%time [Wlist, blist,acc_train_list,acc_test_list]=gradient_descent(weights_list, biases_list, data_norm, mnist.train.labels, sizes, relu, softmax,1,100,0.3,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy=0.972987272727+-0.00174719704689\n",
      "test accuracy=0.96625+-0.0015762296787\n"
     ]
    }
   ],
   "source": [
    "runs=10\n",
    "acc_train=[]\n",
    "acc_test=[]\n",
    "for i in range(runs):\n",
    "    [weights_list, biases_list]=initialise(sizes)\n",
    "    [Wlist, blist,acc_train_list,acc_test_list]=gradient_descent(weights_list, biases_list, data_norm, mnist.train.labels, sizes, relu, softmax,1,100,0.3,0)\n",
    "    acc_train.append(acc_train_list[-1])\n",
    "    acc_test.append(acc_test_list[-1])\n",
    "print(\"training accuracy=\" + str(np.mean(acc_train)) + \"+-\" + str(np.std(acc_train)) )\n",
    "print(\"test accuracy=\" + str(np.mean(acc_test))  + \"+-\" + str(np.std(acc_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary + Comparison to Keras\n",
    "The home-made neural network works and classifies the test images with an accuracy of about $96.5\\pm 0.4$ percent (1 hidden layer with 300 neurons, $\\alpha=0.3, \\,\\lambda=0$, $bsize=100$, using relu for hidden layers and softmax on the output layer). The training on all 55000 images takes 8 seconds for one epoch (on my 3 year old laptop). The code is about 140 lines long, compared to about 10 lines in Keras (see below). The Keras model is faster by (only) a factor 2 and gives test accuracies of only 91 percent after 1 epochs, probably due to a slower (and decaying) learning rate. However, it learns better over additional epochs, reaching 96.5 percent after 10 epochs and then rising further to 97.5 over 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=300, activation='relu', input_dim=784))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.5201 - acc: 0.8493\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.5234 - acc: 0.8500\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.5290 - acc: 0.8471\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.5206 - acc: 0.8511\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 5s 83us/step - loss: 0.5151 - acc: 0.8551\n",
      "10000/10000 [==============================] - 1s 50us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 5s 87us/step - loss: 0.5240 - acc: 0.8492\n",
      "10000/10000 [==============================] - 1s 53us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.5224 - acc: 0.8503\n",
      "10000/10000 [==============================] - 1s 50us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.5159 - acc: 0.8538\n",
      "10000/10000 [==============================] - 1s 50us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 5s 88us/step - loss: 0.5184 - acc: 0.8522\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 5s 88us/step - loss: 0.5294 - acc: 0.8487\n",
      "10000/10000 [==============================] - 1s 53us/step\n",
      "test accuracy=0.913660001397+-0.00162431567208\n"
     ]
    }
   ],
   "source": [
    "#training for 1 epochs, then rebuild model\n",
    "acc_test_keras=[]\n",
    "for i in range(10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=300, activation='relu', input_dim=784))\n",
    "    model.add(Dense(units=10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(data_norm, mnist.train.labels, epochs=1, batch_size=100)\n",
    "    loss_and_metrics = model.evaluate(test_data_norm, mnist.test.labels, batch_size=100)\n",
    "    acc_test_keras.append(loss_and_metrics[1])\n",
    "print(\"test accuracy=\" + str(np.mean(acc_test_keras))  + \"+-\" + str(np.std(acc_test_keras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.5160 - acc: 0.8545\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.2845 - acc: 0.9182\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.2385 - acc: 0.9323\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.2092 - acc: 0.9406\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1879 - acc: 0.9470\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1708 - acc: 0.9525\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.1569 - acc: 0.9564\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.1454 - acc: 0.9601\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.1356 - acc: 0.9626\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.1272 - acc: 0.9651\n",
      "10000/10000 [==============================] - 1s 55us/step\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 5s 87us/step - loss: 0.5184 - acc: 0.8513\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.2868 - acc: 0.9173\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.2391 - acc: 0.9326\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 6s 100us/step - loss: 0.2093 - acc: 0.9412\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.1876 - acc: 0.9470\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.1707 - acc: 0.9513\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 5s 87us/step - loss: 0.1572 - acc: 0.9563\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.1459 - acc: 0.9594\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 5s 87us/step - loss: 0.1362 - acc: 0.9624\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 5s 88us/step - loss: 0.1278 - acc: 0.9650\n",
      "10000/10000 [==============================] - 1s 59us/step\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.5282 - acc: 0.8477\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.2863 - acc: 0.9189\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 5s 93us/step - loss: 0.2382 - acc: 0.9327\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 5s 84us/step - loss: 0.2084 - acc: 0.9414\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 5s 92us/step - loss: 0.1871 - acc: 0.9474\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.1705 - acc: 0.9516\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1572 - acc: 0.9563\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.1461 - acc: 0.9593\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1367 - acc: 0.9623\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.1284 - acc: 0.9651\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.5236 - acc: 0.8514\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.2831 - acc: 0.9201\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.2360 - acc: 0.9328\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.2069 - acc: 0.9416\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.1854 - acc: 0.9483\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.1687 - acc: 0.9527\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 4s 66us/step - loss: 0.1551 - acc: 0.9569\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.1438 - acc: 0.9601\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 4s 67us/step - loss: 0.1342 - acc: 0.9630\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 4s 66us/step - loss: 0.1258 - acc: 0.9655\n",
      "10000/10000 [==============================] - 1s 53us/step\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 6s 113us/step - loss: 0.5268 - acc: 0.8508\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 6s 110us/step - loss: 0.2876 - acc: 0.9171\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 5s 93us/step - loss: 0.2405 - acc: 0.9310\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.2107 - acc: 0.9402\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 5s 89us/step - loss: 0.1891 - acc: 0.9466\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 5s 86us/step - loss: 0.1720 - acc: 0.9514\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 5s 94us/step - loss: 0.1583 - acc: 0.9557\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.1469 - acc: 0.9588\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 5s 84us/step - loss: 0.1369 - acc: 0.9615\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 4s 82us/step - loss: 0.1288 - acc: 0.9641\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "test accuracy=0.961060004234+-0.00125793411429\n"
     ]
    }
   ],
   "source": [
    "#training for 10 epochs, then rebuild model\n",
    "acc_test_keras=[]\n",
    "for i in range(5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=300, activation='relu', input_dim=784))\n",
    "    model.add(Dense(units=10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(data_norm, mnist.train.labels, epochs=10, batch_size=100)\n",
    "    loss_and_metrics = model.evaluate(test_data_norm, mnist.test.labels, batch_size=100)\n",
    "    acc_test_keras.append(loss_and_metrics[1])\n",
    "print(\"test accuracy=\" + str(np.mean(acc_test_keras))  + \"+-\" + str(np.std(acc_test_keras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.5162 - acc: 0.8534\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 4s 69us/step - loss: 0.2867 - acc: 0.9178\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.2422 - acc: 0.9317\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.2143 - acc: 0.9395\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1935 - acc: 0.9458\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 4s 67us/step - loss: 0.1770 - acc: 0.9506\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 4s 66us/step - loss: 0.1633 - acc: 0.9543\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.1520 - acc: 0.9575\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.1422 - acc: 0.9609\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 4s 66us/step - loss: 0.1336 - acc: 0.9634\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.1263 - acc: 0.9654\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.1193 - acc: 0.9673\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 4s 66us/step - loss: 0.1132 - acc: 0.9691\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.1076 - acc: 0.9709\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.1028 - acc: 0.9718\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 4s 66us/step - loss: 0.0981 - acc: 0.9736\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 4s 68us/step - loss: 0.0939 - acc: 0.9746\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.0898 - acc: 0.9758\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.0863 - acc: 0.9767\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 4s 65us/step - loss: 0.0829 - acc: 0.9780\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 4s 67us/step - loss: 0.0797 - acc: 0.9788\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 4s 69us/step - loss: 0.0767 - acc: 0.9799\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0739 - acc: 0.9803\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.0712 - acc: 0.9812\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0687 - acc: 0.9819\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.0665 - acc: 0.9832\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0642 - acc: 0.9835\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.0621 - acc: 0.9842\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 4s 69us/step - loss: 0.0601 - acc: 0.9848\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 4s 69us/step - loss: 0.0582 - acc: 0.9854\n",
      "10000/10000 [==============================] - 1s 56us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97510000467300417"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=300, activation='relu', input_dim=784))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data_norm, mnist.train.labels, epochs=30, batch_size=100)\n",
    "loss_and_metrics = model.evaluate(test_data_norm, mnist.test.labels, batch_size=100)\n",
    "loss_and_metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
