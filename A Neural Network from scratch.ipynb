{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Network from scratch + Comparison to Keras\n",
    "while we can build neural networks (NN) with only a few lines of code using high level Python packages such as Keras/tensorflow, building one from scratch helps making sure we understand everything. I will try to write efficient, vectorised code instead of the much slower for/while loops. The datasets could be anything, so we start with the famous MNIST set of handwritten digits and build a classifier. The result will then be compared to a Keras model.\n",
    "\n",
    "Start by importing some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                                \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from skimage import io\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "n_pix=784, m_train=55000, m_test=10000\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "(n_pix,m_train,m_test)=(len(mnist.train.images[0]),len(mnist.train.images),len(mnist.test.images))\n",
    "print('n_pix='+str(n_pix)+', '+ 'm_train='+str(m_train)+', '+ 'm_test='+str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already separated into m_train train images and m_test test images with corresponding labels. \n",
    "The images are in greyscale (values 0-1) and are given as a one-dimensional vector with length n_pix (=number of pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pix=784, m_train=55000, m_test=10000\n"
     ]
    }
   ],
   "source": [
    "(n_pix,m_train,m_test)=(len(mnist.train.images[0]),len(mnist.train.images),len(mnist.test.images))\n",
    "print('n_pix='+str(n_pix)+', '+ 'm_train='+str(m_train)+', '+ 'm_test='+str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the images by reshaping them into (28x28) arrays and use the io package to display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAABVCAYAAAAcyXCzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE5SIiERJJygREYnS/wP0GSfxMruoLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbc4a1cf390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "piclist=[pic.reshape((28,28)) for pic in mnist.train.images[0:5]]\n",
    "io.imshow_collection(piclist)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 4, 6, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerlist=[np.where(label==1)[0].item() for label in mnist.train.labels[0:5]]\n",
    "answerlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network geometry\n",
    "\n",
    "The geometry of a simple NN consists of an input layer (in our case the 784 pixel values), followed by a number of hidden layers and finally an output layer (with in our case $K=10$ neurons representing the 10 possible digits). Following the deep learning course by Andrew Ng on coursera, the total number of layers (not counting the input) is called $L$, with $s_l$ neurons in layer $l=0...L$, and $s_L=K$ neurons in the output layer.\n",
    "\n",
    "The neurons have values called activations $a^{[l]}$, starting with the input values $a^{[0]}$, the pixel values of our image. To calculate the activation vector $a^{[l]}$ of the neurons in layer $l$, we first multiply the activation vector of the previous layer $a^{[l-1]}$ with the corresponding weight matrix $W^{[l]}$ and add a bias vector $b^{[l]}$, to produce $z^{(l)}= W^{[l]} a^{[l-1]}+b^{[l]}$. The matrices $W^{[l]}$ and the bias vectors $b^{[l]}$ represent the parameters that will be learned through training of the network.\n",
    "\n",
    "The resulting vector $z^{[l]}$ is fed into a nonlinear activation function to give the activation of the neurons of the next layer, $a^{[l]}=g(z^{[l]})$. Historically, the sigmoid function $\\sigma(z)=\\frac{1}{1+exp(-z)}$ was used, or alternatively $tanh(z)$, but recently the computationally easier RELU function $r(z)=max(0,z)$ is the preferred choice for all but the output layer. Here often the softmax function $s(z)_j=\\frac{exp(z_j)}{\\sum_{k=1}^K exp(z_k)}$ is used, which is conveniently normalised ($\\sum s(z)_j=1$) and will be interpreted as probability that the input image belongs to a class.\n",
    "\n",
    "Instead of looping through all $m$ training examples $x^{(i)}$, I will vectorise the process to be more efficient: The data points $x^{(i)}$ will be stored as columns in a matrix $A^{[0]}$, and the corresponding $z$-values will be the columns of the matrix $Z^{[l]}$, with $Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some functions we need and their gradients:\n",
    "\n",
    "def sigmoid(z):\n",
    "    z=np.clip( z, -500, 500 )                       #restrict z to (-500,500) to avoid overflow\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def softmax(z):\n",
    "    z=np.clip( z, -500, 500 )                       #restrict z to (-500,500) to avoid overflow\n",
    "    ex=np.exp(z)                                            \n",
    "    return ex / np.sum(ex, axis=0,keepdims=True)    #column wise sum (collapse column vector to number)\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    sig=sigmoid(z)\n",
    "    return sig*(1-sig)\n",
    "\n",
    "def grad_relu(z):\n",
    "    z=z>0                       #the gradient at zero is not defined, but it's ok here to set it to zero\n",
    "    return z.astype(int)\n",
    "\n",
    "def grad_softmax(z):\n",
    "    grad=np.multiply(softmax(z),np.ones(z.shape)-softmax(z))\n",
    "    return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation and feed forward\n",
    "The process described above is called feed forward, and in practice we initialise the weigth matrices of a network randomly and can feed forward to a first guess (interpreting the activations of the last layer as a measure for likelihoood of the corresponding outcome). We will use either sigmoid or relu as activation in all hidden layers and sigmoid or softmax in the output layer. The list \"sizes\" will contain the numbers of neurons in each layer, $(s_0, ..., s_L)$. For example, sizes=[784,40,10] corresponds to a single hidden layer of 40 neurons between the input layer (784 neurons) and output layer (10 neurons). \n",
    "\n",
    "### Cost function + regularisation\n",
    "Once the activation of the final (=output) layer is calculated, we can compare it to the true label and define a loss or cost function, describing how far off we are. The aim will be to minimise this cost function. The total cost over the training (or test) set is the average of the cost terms $J$ of each data point $(x^{(i)},y^{(i)})$, $J_{tot}=\\frac{1}{m}\\sum_m J$.\n",
    "\n",
    "There are different choices for the cost, for example the cross entropy $J=-\\sum_k y_k\\,log(a_k)+ (1-y_k)\\,log((1-a_k))$, where $a$ is the activation of the output layer and $y$ is the true label vector, with a one marking the correct answer.\n",
    "\n",
    "To prevent overfitting, we regularise the cost by adding a term $\\frac{\\lambda}{2 m}\\sum_l ||W^{[l]}||^2$, which sums up the squared weights of all connections in the network, effectively penalising the algorithm for using large weights. The parameter $\\lambda$ should be chosen carefully.\n",
    "\n",
    "### Training the network: Back propagation and gradient descent\n",
    "We now minimise the cost by calculating the gradient of the cost with respect to the weights and biases. For this, we change the weights and biases proportional to a learning rate $\\alpha$ in the direction of the partial gradients of the cost function: $W^{[l]}\\rightarrow W^{[l]}- \\alpha\\frac{\\partial J}{ \\partial W^{[l]}}$ and $b^{[l]}\\rightarrow b^{[l]}- \\alpha \\frac{\\partial J}{ \\partial b^{[l]}}$. The process can be compared with trying to get to the top of a mountain by always taking a step (with step size $\\alpha$) in the steepest direction. If $\\alpha$ is too large we might step too far, but if $\\alpha$ is too small, it will take forever and we might get stuck on a small hill.\n",
    "\n",
    "So how do we get the gradients? Just like we found the final activation layer by propagating forward through the layers, we now propagate backwards to get all derivatives: We start with the last layer, $\\frac{\\partial J}{ \\partial W^{[L]}}=\\frac{\\partial J}{ \\partial z^{[L]}}\\frac{\\partial z^{[L]}}{ \\partial W^{[L]}}$ with $\\frac{\\partial z^{[L]}}{ \\partial W^{[L]}}=a^{[L-1]}$ and the \"error\" of layer $L$, defined as $\\delta^{[L]}:=\\frac{\\partial J}{ \\partial z^{[L]}}$. All partial derivatives with respect to a vector are to be understood elementwise (e.g. $\\delta_j^{[L]}:=\\frac{\\partial J}{ \\partial z_j^{[L]}}$), and so is the multiplication between the vectors.\n",
    "\n",
    "Next, we calculate the errors from $\\delta^{[L]}=\\frac{\\partial J}{ \\partial a^{[L]}}\\frac{\\partial a^{[L]}}{ \\partial z^{[L]}}$ with $\\frac{\\partial J}{ \\partial a_k}=\\frac{\\partial }{ \\partial a_k} \\left( -\\sum_{k'} y_{k'}\\,log(a_{k'})+ (1-y_{k'})\\,log((1-a_{k'})) \\right)=-\\frac{y_k}{a_k}+\\frac{1-y_k}{1-a_k}=\\frac{a_k-y_k}{a_k(1-a_k)}$ for cross entropy and $\\frac{\\partial a^{[L]}}{\\partial z^{[L]}}=\\frac{\\partial}{ \\partial z^{[L]}}g(z^{[L]})=g'(z^{[L]})$. If $g=softmax$, we get $g'(z)=softmax(z)\\,(1-softmax(z))=a \\,(1-a)$ since $a=g(z)$, and similarly if $g=sigmoid$, $g'(z)=sigmoid(z)\\,(1-sigmoid(z))=a \\,(1-a)$. In both cases the error reduces to $\\delta^{[L]}=a^{[L]}-y$.\n",
    "\n",
    "Finally, the error of layer ($l-1$) can be calculated from the error of layer $l$ via $\\delta^{[l-1]}=(W^{[l]})^T   \\delta^{[l]}\\, g'(z^{[l-1]})$, where $W^T$ is the transposed weight matrix. For the biases, we simply have $\\frac{\\partial J}{ \\partial b^{[l]}}=\\delta^{[l]}$.\n",
    "\n",
    "Another implementation detail is the Mini-batch method. In backpropagation, we calculate the gradient of the cost function with respect to $b$ or $W$ by averaging over all training examples. This can take long, so instead a common method is to select a random subset called \"mini-batch\" of $n<m$ training examples and take the average gradient of those. As a result, the algorithm will be changing the weights and biases in a sometimes less optimum way, which might in rare cases even increase the cost, but eacch step will be done much faster. Typical step size could be $n=10-100$, compared to $m=55000$ in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myNetwork:\n",
    "    \n",
    "    def __init__(self, sizes, activation, activation_final):  # store pameters of the network\n",
    "        self.sizes=sizes\n",
    "        self.biases=[np.zeros((s, 1)) for s in sizes[1:]]\n",
    "        self.weights=[np.sqrt(2/sizes[0])*np.random.randn(s2, s1) for s1, s2 in zip(sizes[:-1], sizes[1:])]     #gaussian weights\n",
    "        self.activation=activation\n",
    "        self.activation_final=activation_final\n",
    "    \n",
    "    def gradient_descent(self, A, Y, A_test, Y_test, epochs, bsize, alpha, lmbda):\n",
    "    \n",
    "        m=len(A.T)\n",
    "        A_test_norm=((A_test-np.mean(A))/np.std(A)) #normalise test and training data (both in the same way):\n",
    "        A=(A-np.mean(A))/np.std(A)\n",
    "    \n",
    "        A_in=A                                  #store for evaluation\n",
    "        Y_in=Y                                  \n",
    "    \n",
    "        acc_train_list=[]\n",
    "        acc_test_list=[]\n",
    "          \n",
    "        for e in range(epochs):                           #loop through forward- and backprop\n",
    "        \n",
    "            [Ashuffled,Yshuffled]=self.shuffle(A,Y)            #Ashuffled (784 x m), Yshuffled (10 x m)\n",
    "            nb=int(np.floor(m/bsize))                     # number of minibatches per epoch\n",
    "        \n",
    "            for b in range(nb):\n",
    "             \n",
    "                A=Ashuffled[:,(b*bsize):((b+1)*bsize)]    #(784 x bsize)\n",
    "                Y=Yshuffled[:,(b*bsize):((b+1)*bsize)]    #(10 x bsize)\n",
    "            \n",
    "                #feed forward to get activations:\n",
    "                [Zlist,Alist]=self.feedforward(A)\n",
    "            \n",
    "                #backprop:      \n",
    "                [dWlist,dblist]=self.backprop(Alist,Zlist,Y, bsize)\n",
    "                      \n",
    "                #update weights and biases (with L2 regularisation):                        \n",
    "                [self.weights,self.biases]=self.update(dWlist,dblist,alpha,lmbda)\n",
    "                    \n",
    "            acc_train=self.accuracy(A_in,Y_in)\n",
    "            acc_test=self.accuracy(A_test_norm,Y_test)\n",
    "            acc_train_list.append(acc_train)\n",
    "            acc_test_list.append(acc_test)\n",
    "        \n",
    "        return [self.weights, self.biases,acc_train_list,acc_test_list]\n",
    "    \n",
    "    def feedforward(self, A):\n",
    "        Alist=[A]\n",
    "        Zlist=[]\n",
    "    \n",
    "        for k in range(len(self.weights)-1):                  # relu or sigmoid as activation for hidden layers\n",
    "            Z=np.dot(self.weights[k], A)+self.biases[k]            #add same vector b to each column of matrix (\"broadcasting\")\n",
    "            A = self.activation(Z)\n",
    "            Zlist.append(Z)\n",
    "            Alist.append(A)\n",
    "        \n",
    "        Z=np.dot(self.weights[-1], A)+self.biases[-1]\n",
    "        A = self.activation_final(Z)                         #softmax or sigmoid as activation for final layer\n",
    "        Alist.append(A)\n",
    "        Zlist.append(Z)\n",
    "\n",
    "        return [Zlist,Alist]                            #return the activations and z's\n",
    "    \n",
    "    def backprop(self, Alist,Zlist,Y, bsize):        #single backpropagation\n",
    "    \n",
    "        if str(self.activation)==str(sigmoid):       # allow two different activation functions for the hidden layers\n",
    "            grad_activation=grad_sigmoid\n",
    "        elif str(self.activation)==str(relu):\n",
    "            grad_activation=grad_relu\n",
    "        else: print(\"check activation\")  \n",
    "\n",
    "        L=len(self.sizes)-1                                  #don't count the input layer\n",
    "        dWlist=[]                                         # list to save weights errors in each mini-batch run\n",
    "        dblist=[] \n",
    "    \n",
    "        #first backprop step in output layer (sigmoid or softmax):\n",
    "        dZ=Alist[-1]-Y                                                 # dZ2:(10 x bsize), A2:(10 x bsize)\n",
    "        dW=1/bsize*np.dot(dZ, Alist[-2].T)                             #dW2:(10 x s1), A1:(s1 x bsize)\n",
    "        db=1/bsize*np.sum(dZ,axis=1,keepdims=True)                      #db2:(10 x 1)\n",
    "        dWlist.append(dW)\n",
    "        dblist.append(db)     \n",
    "                      \n",
    "        #step backwards through hidden layers (sigmoid or relu)\n",
    "        for j in range(L-1):\n",
    "            xz=np.dot(self.weights[-(j+1)].T,dZ)                        #xz:(s1 x bsize)                      \n",
    "            dZ=np.multiply(xz, grad_activation(Zlist[-(j+2)]))          #dZ1:(s1 x bsize)\n",
    "            dW=1/bsize*np.dot(dZ, Alist[-(j+3)].T)                      #dW1:(s1 x 784)\n",
    "            db=1/bsize*np.sum(dZ,axis=1,keepdims=True)                                \n",
    "            dWlist.append(dW)\n",
    "            dblist.append(db)\n",
    "    \n",
    "        return [dWlist,dblist]\n",
    "    \n",
    "    def update(self,dWlist,dblist,alpha,lmbda):     # updates weights and biases with L2 regularisation\n",
    "        for j in range(len(dWlist)):\n",
    "            self.weights[-(j+1)]=(1-alpha*lmbda)*self.weights[-(j+1)]-alpha*dWlist[j]\n",
    "        for k in range(len(dblist)):\n",
    "            self.biases[-(k+1)]=self.biases[-(k+1)]-alpha*dblist[k]\n",
    "        return [self.weights, self.biases]\n",
    "    \n",
    "    def shuffle(self,A,Y):                          #shuffles columns of images and labels\n",
    "        shuffled=np.vstack((A,Y))                     \n",
    "        np.random.shuffle(shuffled.T)                 \n",
    "        return [shuffled[0:-10,:],shuffled[-10:,:]]\n",
    "    \n",
    "    def accuracy(self, A, Y):  #check prediction accuracy on test (or training) data\n",
    "        A_final=self.feedforward(A)[1][-1]\n",
    "        predict_list=A_final.argmax(axis=0)    \n",
    "        true_list=Y.argmax(axis=0)\n",
    "        return sum(predict_list==true_list)/len(predict_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network: Hyper parameters, Bias and Variance\n",
    "\n",
    "Now that the algorithm is set up, we can train the model parameters (weights and biases) for different sets of hyper parameters (choice of activation function, number of neurons and hidden layers, learning rate, regularisation). I will mostly work with one hidden layer, and start without regularisation. \n",
    "\n",
    "In order to help finding the right parameters, it helps to know about the high bias/ high variance problem. In a high bias situation, the model doesn't have enough degrees of freedom to make good fits to the data (imagine fitting a linear function to quadraticly looking data). Here, additional data will not help, but instead the model should be made more complex (e.g. more neuron, more layers). If instead we have high variance, the model fits perfecctly to the training data but fails to generalise well to also classify the test data. This \"over-fitting\" can be avoided by adding more data or other techniques such as regularisation (e.g. increase $\\lambda$) or drop out.\n",
    "\n",
    "### Learning rate decay\n",
    "\n",
    "Another trick in stochastic gradient descent is to slowly reduce the learning rate or to use an update that depends on the mean or variance of the past gradients (e.g. see \"Adam\" or \"RMSprop\"). This idea can be imagined as as marble rolling down the cost-surface, which would still keep rolling in a locally flat region due to the momentum (or speed) collected in the past steeper regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A=mnist.train.images.T                        # we want the m training examples stored in columns, not rows\n",
    "Y=mnist.train.labels.T\n",
    "A_test=mnist.test.images.T\n",
    "Y_test=mnist.test.labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy=0.973432727273+-0.00219777197473\n",
      "test accuracy=0.96668+-0.00221033934046\n"
     ]
    }
   ],
   "source": [
    "acc_train=[]\n",
    "acc_test=[]\n",
    "runs=5           # run multiple times to see if random initialisation changes accuracy\n",
    "\n",
    "for i in range(runs):\n",
    "    net00=myNetwork([784,400,10], relu, softmax)  #initialise network\n",
    "    [Wlist, blist,acc_train_list,acc_test_list]=net00.gradient_descent(A, Y, A_test,Y_test, 1,100,0.3,0)\n",
    "    acc_train.append(acc_train_list[-1])\n",
    "    acc_test.append(acc_test_list[-1])\n",
    "    \n",
    "print(\"training accuracy=\" + str(np.mean(acc_train)) + \"+-\" + str(np.std(acc_train)) )\n",
    "print(\"test accuracy=\" + str(np.mean(acc_test))  + \"+-\" + str(np.std(acc_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary + Comparison to Keras\n",
    "The home-made neural network works and classifies the test images with an accuracy of about $96.7$ percent (1 hidden layer with 400 neurons, $\\alpha=0.3, \\,\\lambda=0$ (didn't help), $bsize=100$, using relu for hidden layers and softmax on the output layer). The training on all 55000 images takes 8 seconds for one epoch (on my 3 year old laptop). The code is about 110 lines long, compared to about 10 lines in Keras (see below). The Keras model is faster by (only) a factor 2 and gives test accuracies of only 91 percent after 1 epoch, probably due to a slower (and decaying) learning rate. However, it learns better over additional epochs, reaching 96.5 percent after 10 epochs and then rising further to 97.5 over 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=300, activation='relu', input_dim=784))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 134us/step - loss: 0.5102 - acc: 0.8555\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 123us/step - loss: 0.4981 - acc: 0.8575\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 129us/step - loss: 0.5076 - acc: 0.8542\n",
      "10000/10000 [==============================] - 1s 55us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 133us/step - loss: 0.5003 - acc: 0.8598\n",
      "10000/10000 [==============================] - 1s 56us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 127us/step - loss: 0.5061 - acc: 0.8574\n",
      "10000/10000 [==============================] - 1s 56us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 133us/step - loss: 0.5063 - acc: 0.8581\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 133us/step - loss: 0.5142 - acc: 0.8542\n",
      "10000/10000 [==============================] - 1s 59us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 134us/step - loss: 0.5004 - acc: 0.8612\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 7s 128us/step - loss: 0.5098 - acc: 0.8538\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 8s 152us/step - loss: 0.5082 - acc: 0.8560\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "test accuracy=0.915550003052+-0.0017704515308\n"
     ]
    }
   ],
   "source": [
    "#training for 1 epoch, then rebuild model\n",
    "acc_test_keras=[]\n",
    "A_norm=((A-np.mean(A))/np.std(A)).T\n",
    "A_test_norm=((A_test-np.mean(A))/np.std(A)).T\n",
    "for i in range(10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=400, activation='relu', input_dim=784))\n",
    "    model.add(Dense(units=10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(A_norm, mnist.train.labels, epochs=1, batch_size=100)\n",
    "    loss_and_metrics = model.evaluate(A_test_norm, mnist.test.labels, batch_size=100)\n",
    "    acc_test_keras.append(loss_and_metrics[1])\n",
    "print(\"test accuracy=\" + str(np.mean(acc_test_keras))  + \"+-\" + str(np.std(acc_test_keras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
